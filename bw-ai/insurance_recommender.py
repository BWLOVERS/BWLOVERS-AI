import json
import os
import uuid
import re
from typing import Dict, Any, List, Optional
from datetime import datetime

# FAISS ê¸°ë°˜ RAG + LLM ê´€ë ¨ ì„í¬íŠ¸
try:
    from langchain_community.vectorstores import FAISS
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_core.documents import Document

    # rag_pipeline import (LLM + í”„ë¡¬í”„íŠ¸)
    from rag_pipeline import ask_question

    # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
    embeddings = HuggingFaceEmbeddings(
        model_name="jhgan/ko-sroberta-multitask",
        model_kwargs={"device": "cpu"},
        encode_kwargs={"normalize_embeddings": True},
    )

    # FAISS ë²¡í„°ìŠ¤í† ì–´ ì ˆëŒ€ ê²½ë¡œ ì„¤ì •
    CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
    faiss_path = os.path.join(CURRENT_DIR, "..", "faiss_index")

    # FAISS ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ
    if os.path.exists(os.path.join(faiss_path, "index.faiss")):
        vectorstore = FAISS.load_local(faiss_path, embeddings, allow_dangerous_deserialization=True)
        print(f"âœ… ê¸°ì¡´ FAISS ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œë¨: {vectorstore.index.ntotal}ê°œ ë¬¸ì„œ")
    else:
        vectorstore = None
        print("ğŸ†• FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì˜ˆì •")

    RAG_AVAILABLE = True
    LLM_AVAILABLE = True

except Exception as e:
    print(f"âŒ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    vectorstore = None
    embeddings = None
    RAG_AVAILABLE = False
    LLM_AVAILABLE = False
    CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))

# ë³´í—˜ë£Œ ë° ê°€ì…ê¸ˆì•¡ í…Œì´ë¸” ë¡œë“œ
PRICE_MAP = {}
SUM_INSURED_MAP = {}
PRICE_FILE = os.path.join(CURRENT_DIR, "prices.json")
SUM_INSURED_FILE = os.path.join(CURRENT_DIR, "sum_insured.json")


def _load_data_maps():
    global PRICE_MAP, SUM_INSURED_MAP
    for file_path, target_map, name in [
        (PRICE_FILE, PRICE_MAP, "ë³´í—˜ë£Œ"),
        (SUM_INSURED_FILE, SUM_INSURED_MAP, "ê°€ì…ê¸ˆì•¡"),
    ]:
        if os.path.exists(file_path):
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    target_map.update(json.load(f))
                print(f"âœ… {name} í…Œì´ë¸” ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                print(f"âŒ {name} ë¡œë“œ ì‹¤íŒ¨: {e}")


_load_data_maps()


# ---- ë³´í—˜ì‚¬ëª… ì¶”ì¶œ(ì˜¤ë‹µ ë°©ì§€ìš©) ----
INSURER_NAMES = [
    "ì‚¼ì„±í™”ì¬",
    "í˜„ëŒ€í•´ìƒ",
    "DBì†í•´ë³´í—˜",
    "KBì†í•´ë³´í—˜",
    "ë©”ë¦¬ì¸ í™”ì¬",
    "í•œí™”ì†í•´ë³´í—˜",
    "í¥êµ­í™”ì¬",
    "ë¡¯ë°ì†í•´ë³´í—˜",
    "MGì†í•´ë³´í—˜",
]


def extract_insurer_name(text: Optional[str]) -> str:
    if not text:
        return ""
    for name in INSURER_NAMES:
        if name in text:
            return name
    return ""


def looks_like_plan_name(s: str) -> bool:
    if not s:
        return False
    # ë³´í—˜ìƒí’ˆëª…(í”Œëœëª…)ì—ì„œ í”íˆ ë³´ì´ëŠ” í‚¤ì›Œë“œ
    return any(k in s for k in ["ë¬´ë°°ë‹¹", "ë‹¤ì´ë ‰íŠ¸", "í•´ì•½í™˜ê¸‰ê¸ˆ", "ë³´í—˜", "í˜•", "ë³´ì¥"]) or len(s) >= 15


def looks_like_contract_name(s: str) -> bool:
    if not s:
        return False
    # íŠ¹ì•½/ë‹´ë³´ëª…ì—ì„œ í”íˆ ë³´ì´ëŠ” í‚¤ì›Œë“œ
    return any(k in s for k in ["íŠ¹ì•½", "íŠ¹ë³„ì•½ê´€", "ì§„ë‹¨ë¹„", "ì‹¤ì†", "ìœ„ë¡œê¸ˆ", "ì…ì›ì˜ë£Œë¹„"])


class InsuranceRecommender:
    def __init__(self):
        self.vectorstore = vectorstore
        self.embeddings = embeddings
        if RAG_AVAILABLE:
            self._load_insurance_data()

    def _load_insurance_data(self):
        """ì ˆëŒ€ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  JSON ë°ì´í„°ë¥¼ FAISSì— ë¡œë“œ"""
        try:
            # ì´ë¯¸ ë¡œë“œë˜ì—ˆë‹¤ë©´ ìŠ¤í‚µ
            if self.vectorstore and self.vectorstore.index.ntotal > 0:
                return

            documents = []
            data_dir = os.path.join(CURRENT_DIR, "..", "json", "Llama_json")

            if not os.path.exists(data_dir):
                print(f"âŒ ë°ì´í„° ë””ë ‰í† ë¦¬ ì—†ìŒ: {data_dir}")
                return

            json_files = [f for f in os.listdir(data_dir) if f.endswith(".json")]
            print(f"ğŸ“‚ {len(json_files)}ê°œ íŒŒì¼ ë¶„ì„ ì¤‘...")

            for filename in json_files:
                filepath = os.path.join(data_dir, filename)
                with open(filepath, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    items = data if isinstance(data, list) else [data]
                    for item in items:
                        content = item.get("content", "").strip()
                        if content and len(content) > 20:
                            doc = Document(
                                page_content=content,
                                metadata={**item.get("metadata", {}), "source_file": filename},
                            )
                            documents.append(doc)

            if documents:
                self.vectorstore = FAISS.from_documents(documents, self.embeddings)
                os.makedirs(faiss_path, exist_ok=True)
                self.vectorstore.save_local(faiss_path)
                print(f"âœ… FAISS ìƒì„± ì™„ë£Œ: {len(documents)}ê°œ ë¬¸ì„œ")
        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¡œë“œ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")

    def search_relevant_documents(self, query: str, n_results: int = 10):
        if not self.vectorstore:
            print("âš ï¸ ê²€ìƒ‰ ë¶ˆê°€: ë²¡í„°ìŠ¤í† ì–´ê°€ ë¹„ì–´ìˆìŒ")
            return []
        try:
            docs_with_scores = self.vectorstore.similarity_search_with_score(query, k=n_results)
            return [doc for doc, score in docs_with_scores]
        except Exception as e:
            print(f"âŒ FAISS ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def generate_rag_recommendation(self, user_profile: Dict[str, Any], health_status: Dict[str, Any]) -> Dict[str, Any]:
        try:
            # 1. ì‚¬ìš©ì ë¶„ì„
            analysis = self._analyze_user_profile(user_profile, health_status)

            # 2. ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± ë° ë¬¸ì„œ ê²€ìƒ‰
            search_query = self._build_rag_query(analysis)
            relevant_docs = self.search_relevant_documents(search_query, n_results=12)

            if not relevant_docs:
                print("âš ï¸ ê²€ìƒ‰ëœ ë¬¸ì„œ ì—†ìŒ -> Fallback ì‹¤í–‰")
                return self._fallback_recommendation(user_profile, health_status)

            # 3. LLM ì§ˆë¬¸ ìƒì„± ë° í˜¸ì¶œ
            context = self._build_context_from_documents(relevant_docs)
            llm_question = self._build_llm_question(analysis, context)

            print(f"ğŸ¤– LLM ìš”ì²­ ì¤‘... (ì£¼ìˆ˜: {analysis['gestational_week']}ì£¼)")
            rag_result = ask_question(llm_question, profile=analysis)

            if rag_result and "answer" in rag_result:
                result = self._parse_llm_response_to_recommendation(rag_result["answer"], analysis, relevant_docs)
                if not result.get("items"):
                    return self._fallback_recommendation(user_profile, health_status)
                return result

            return self._fallback_recommendation(user_profile, health_status)
        except Exception as e:
            print(f"âŒ RAG í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨: {e}")
            return self._fallback_recommendation(user_profile, health_status)

    def _build_rag_query(self, analysis: Dict[str, Any]) -> str:
        parts = ["ì„ì‹  ë³´í—˜", "íƒœì•„ ë³´ì¥"]
        week = analysis.get("gestational_week", 0)
        if week > 0:
            parts.append(f"{week}ì£¼")
        if analysis.get("is_multiple_pregnancy"):
            parts.append("ë‹¤íƒœì•„ ìŒë‘¥ì´")
        if analysis.get("risk_factors"):
            parts.extend(analysis.get("risk_factors")[:2])
        return " ".join(parts)

    def _build_context_from_documents(self, documents) -> str:
        parts = []
        for i, doc in enumerate(documents[:8]):  # 8ê°œë¡œ ì œí•œ
            md = doc.metadata or {}
            parts.append(
                f"[ë¬¸ì„œ {i+1}] ìƒí’ˆ:{md.get('product_name','?')}, í˜ì´ì§€:{md.get('page_number','?')}\n"
                f"ë‚´ìš©:{doc.page_content[:800]}"
            )
        return "\n\n".join(parts)

    # âœ… í”„ë¡¬í”„íŠ¸(ì¶œë ¥ ìŠ¤í‚¤ë§ˆ) ìŠ¤í™ëŒ€ë¡œ ìˆ˜ì •í•¨
    def _build_llm_question(self, analysis: Dict[str, Any], context: str) -> str:
        return f"""
ì—­í• : ë³´í—˜ ì „ë¬¸ ì–¸ë”ë¼ì´í„°
ì„ì‹ ë¶€ ì •ë³´: {analysis['gestational_week']}ì£¼ì°¨, ìœ„í—˜ìš”ì¸({analysis.get('risk_factors', [])}), ë‹¤íƒœì•„({analysis['is_multiple_pregnancy']})

ì§€ì¹¨:
1. ì œê³µëœ [ë³´í—˜ ì•½ê´€ ì •ë³´]ë§Œ ê·¼ê±°ë¡œ ê°€ì¥ ì í•©í•œ ë³´í—˜ ìƒí’ˆ 2-3ê°œë¥¼ ì¶”ì²œí•˜ë¼.
2. ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•˜ë¼. (ì„¤ëª… ë¬¸ì¥/ì½”ë“œë¸”ë¡/ì£¼ì„ ê¸ˆì§€)
3. evidenceëŠ” ë¬¸ë§¥ì—ì„œ ê·¸ëŒ€ë¡œ ì¸ìš©í•œ ë¬¸ì¥ê³¼ í˜ì´ì§€ë¥¼ í¬í•¨í•˜ë¼.
4. ë§¤ìš° ì¤‘ìš”:
   - insurance_companyì—ëŠ” "ì‚¼ì„±í™”ì¬"ì²˜ëŸ¼ 'ë³´í—˜ì‚¬ëª…'ë§Œ ì‘ì„±
   - product_nameì—ëŠ” "ë¬´ë°°ë‹¹ ... ë³´í—˜(...)"ì²˜ëŸ¼ 'ë³´í—˜ìƒí’ˆëª…'ë§Œ ì‘ì„±
   - íŠ¹ì•½ëª…ì€ special_contracts ë°°ì—´ì—ë§Œ ì‘ì„± (product_nameì— íŠ¹ì•½ëª… ì“°ì§€ ë§ ê²ƒ)

[ë³´í—˜ ì•½ê´€ ì •ë³´]
{context}

ì¶œë ¥ í˜•ì‹(ë°˜ë“œì‹œ ì´ í‚¤ë¡œë§Œ):
{{
  "recommendations": [
    {{
      "insurance_company": "ë³´í—˜ì‚¬ëª…",
      "product_name": "ë³´í—˜ìƒí’ˆëª…",
      "monthly_cost": 30000,
      "reason": "ì£¼ìˆ˜ì™€ ìœ„í—˜ìš”ì¸ì„ ê³ ë ¤í•œ êµ¬ì²´ì  ì¶”ì²œ ì´ìœ ",
      "special_contracts": ["íŠ¹ì•½ëª…1", "íŠ¹ì•½ëª…2"],
      "evidence": "ì¸ìš©ë¬¸... (page=ìˆ«ì)"
    }}
  ]
}}
"""

    def _parse_llm_response_to_recommendation(self, llm_response: str, analysis: Dict[str, Any], relevant_docs) -> Dict[str, Any]:
        try:
            json_block = re.search(r"(\{.*\})", llm_response, re.DOTALL)
            if not json_block:
                return {"items": []}

            data = json.loads(self._fix_json_string(json_block.group(1)))
            recs = data.get("recommendations", [])

            items = []
            for idx, rec in enumerate(recs[:3]):
                doc = relevant_docs[idx] if idx < len(relevant_docs) else relevant_docs[0]
                md = doc.metadata or {}

                # âœ… ìŠ¤í™ í‚¤ë¡œ íŒŒì‹±
                comp = (rec.get("insurance_company") or "").strip()
                prod = (rec.get("product_name") or "").strip()

                # âœ… ë°©ì–´: LLMì´ ë˜ ë’¤ì§‘ì–´ ì“°ëŠ” ê²½ìš° êµì •
                # - compê°€ í”Œëœëª…ì²˜ëŸ¼ ë³´ì´ê³  prodê°€ íŠ¹ì•½ì²˜ëŸ¼ ë³´ì´ë©´
                #   -> prodëŠ” comp(í”Œëœëª…)ë¡œ, compëŠ” í”Œëœëª…ì—ì„œ ë³´í—˜ì‚¬ëª…ë§Œ ì¶”ì¶œ
                if looks_like_plan_name(comp) and looks_like_contract_name(prod):
                    plan_name = comp
                    comp = extract_insurer_name(plan_name) or comp  # ìµœì•…ì˜ ê²½ìš° ì›ë³¸ ìœ ì§€
                    prod = plan_name

                # ê°€ì…ê¸ˆì•¡ ë° ë³´í—˜ë£Œ í…Œì´ë¸” ë§¤ì¹­(ë³´í—˜ì‚¬+ìƒí’ˆëª… ê¸°ì¤€)
                sum_insured = self._get_sum_insured(comp, prod)
                monthly_cost = self._get_insurance_price(comp, prod)

                special_contracts = rec.get("special_contracts", []) or []
                # í˜¹ì‹œ LLMì´ special_contractsë¥¼ ë¬¸ìì—´ë¡œ ì¤„ ìˆ˜ë„ ìˆì–´ì„œ ë°©ì–´
                if isinstance(special_contracts, str):
                    special_contracts = [special_contracts]

                items.append({
                    "itemId": uuid.uuid4().hex[:8],
                    "insurance_company": comp,
                    "product_name": prod,
                    "is_long_term": True,
                    "sum_insured": int(sum_insured),
                    "monthly_cost": str(monthly_cost),
                    "insurance_recommendation_reason": rec.get("reason", ""),
                    "special_contracts": [
                        {
                            "contract_name": str(c),
                            "contract_description": "ì•½ê´€ ê¸°ë°˜ ë§ì¶¤ ë³´ì¥",
                            "contract_recommendation_reason": f"{analysis['gestational_week']}ì£¼ì°¨ ë§ì¶¤ íŠ¹ì•½",
                            "key_features": ["ë³´ì¥ ë²”ìœ„ í™•ì¸ ì™„ë£Œ"],
                            "page_number": int(md.get("page_number", 1))
                        } for c in special_contracts
                    ],
                    "evidence_sources": [
                        {
                            "page_number": int(md.get("page_number", 1)),
                            "text_snippet": rec.get("evidence", "")
                        }
                    ],
                })

            return {
                "resultId": uuid.uuid4().hex[:8],
                "items": items,
                "rag_metadata": {
                    "documents_used": len(relevant_docs),
                    "gestational_week": analysis["gestational_week"],
                },
            }
        except Exception as e:
            print(f"âŒ LLM ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return {"items": []}

    def _fix_json_string(self, s: str) -> str:
        s = s.replace("ã€Œ", "'").replace("ã€", "'").replace("â€œ", "'").replace("â€", "'")
        return s.replace("True", "true").replace("False", "false").replace("None", "null")

    def _get_sum_insured(self, c, p):
        return SUM_INSURED_MAP.get(c, {}).get(p, 10000000)

    def _get_insurance_price(self, c, p):
        return PRICE_MAP.get(c, {}).get(p, 30000)

    def _fallback_recommendation(self, up, hs):
        return {"resultId": "fallback", "items": [], "rag_metadata": {"fallback": True}}

    def _analyze_user_profile(self, user_profile: Dict[str, Any], health_status: Dict[str, Any]) -> Dict[str, Any]:
        """Javaì˜ ë‹¤ì–‘í•œ í•„ë“œëª…(Camel/Snake) ì™„ë²½ ëŒ€ì‘"""

        p_info = user_profile.get("pregnancyInfo") or user_profile

        gest_week = p_info.get("gestationalWeek") or p_info.get("gestational_week") or 0
        is_multiple = p_info.get("isMultiplePregnancy") or p_info.get("is_multiple_pregnancy") or False
        miscarriage = p_info.get("miscarriageHistory") or p_info.get("miscarriage_history") or 0

        analysis = {
            "gestational_week": int(gest_week),
            "is_multiple_pregnancy": bool(is_multiple),
            "miscarriage_history": int(miscarriage),
            "risk_factors": [],
        }

        comps = health_status.get("pregnancyComplications") or health_status.get("pregnancy_complications") or []
        for c in comps:
            c_type = c if isinstance(c, str) else (c.get("pregnancyComplicationType") or c.get("complication_type"))
            if c_type == "PREECLAMPSIA":
                analysis["risk_factors"].append("ì„ì‹ ì¤‘ë…ì¦")
            elif c_type == "PRETERM_RISK":
                analysis["risk_factors"].append("ì¡°ì‚°ìœ„í—˜")

        return analysis


recommender = InsuranceRecommender()
